Logistic regression 
is a well-known algorithm for classification tasks. Although it's mentioned in the name, this algorithm can't be used for regression tasks.


The object  𝑒
is constant called Euler's number and its value is roughly 2.71


The Sigmoid function
 is that it only outputs continuous values between 0 and 1. 


Collinearity
In statistics, multicollinearity or collinearity is a situation where the predictors in a regression model are linearly dependent. 
Perfect multicollinearity refers to a situation where the predictive variables have an exact linear relationship. 


R²
The coefficient of determination 
measures how well a statistical model predicts an outcome. The outcome is represented by the model’s dependent variable.
The lowest possible value of R² is 0 and the highest possible value is 1. 
Put simply, the better a model is at making predictions, the closer its R² will be to 1.


One-hot encoding
is used to represent categorical features with new binary features that only contain `0` and `1`.


ROC curve
describes how the recall and false positive rate change with the threshold value.

ROC AUC
Receiver operator characteristic area under the curve



Random forest 
is a commonly-used machine learning algorithm that combines the output of multiple decision trees to reach a single result.

Gini impurity
This is the probability of misclassifying a data point based on this feature
0 would indicate that all the data points belong to the same class


criterion parameter
controls which criterion is used to achieve the purest possible leaves
You can choose between 'gini' and 'entropy'. These two criteria give the same results in 98% of cases, but 'entropy' can use more computing resources, 
which is why the default 'criterion='gini' is often used.


Bootstrap aggregating, 
also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of 
machine learning algorithms used in statistical classification and regression.

Random feature selection: Each time you want a decision tree to generate a decision rule, only a few randomly selected features are available to it.
How many features do you want? Generally,  𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠⎯⎯√
(rounded down) is chosen for this. For the attrition data set, this would be  √⎯⎯15=3.87, 
so we round down to 3 features. This means that when the decision tree generates each decision rule, it uses the best of three randomly selected features to distinguish the categories from each other.


